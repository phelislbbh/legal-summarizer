{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legal Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import textract\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import corpus, pos_tag \n",
    "import gensim \n",
    "import enchant\n",
    "import re\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import logging\n",
    "import pprint\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter( indent = 4, width = 150 )\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the english dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = enchant.Dict( \"en_US\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all required methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_path( path ) :\n",
    "    \n",
    "    for root, dirs, files in os.walk( path ) :\n",
    "        \n",
    "        for filename in files :\n",
    "            \n",
    "            yield root + \"/\" + filename\n",
    "            \n",
    "            \n",
    "def load_text_from_file( filename ) :\n",
    "    \n",
    "    contents = textract.process( filename, encoding='ascii' )\n",
    "    contents = re.sub( '\\\\n',' ', str( contents ) )\n",
    "    \n",
    "    return contents\n",
    "\n",
    "\n",
    "def delete_dir_contents( path ) :\n",
    "    \n",
    "    shutil.rmtree( path )\n",
    "    \n",
    "    os.makedirs( path )\n",
    "    \n",
    "    \n",
    "def number_of_files_in_path( path ) :\n",
    "    \n",
    "    return len( next( os.walk( path ) )[ 2 ] )\n",
    "\n",
    "\n",
    "def get_file_extension( filename ) :\n",
    "    \n",
    "    return os.path.splitext( filename )[ 1 ]\n",
    "\n",
    "\n",
    "def get_file_basename( filename ) :\n",
    "    \n",
    "    return os.path.basename( filename )\n",
    "\n",
    "\n",
    "def is_supported_file_type( filename ) :\n",
    "    \n",
    "    # A list of all supported extensions\n",
    "    valid_extensions = [ \".doc\", \".docx\", \".pdf\", \".rtf\" ]\n",
    "    \n",
    "    return valid_extensions.count( get_file_extension( filename ) ) == 1\n",
    "\n",
    "\n",
    "def generate_next_filename( path, extension ) :\n",
    "    \n",
    "    return path + \"/\" + str( number_of_files_in_path( path ) + 1 ) + extension\n",
    "\n",
    "\n",
    "def write_data_to_file( filename, data ) :\n",
    "    \n",
    "    # Open the file for writing\n",
    "    file = open( filename, 'w' )\n",
    "    \n",
    "    # Write the data to the file\n",
    "    file.write( data )\n",
    "    \n",
    "    # Close the file\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def generate_raw_files( source_dir, target_dir ) :\n",
    "    \n",
    "    # Clear all existing raw cases\n",
    "    delete_dir_contents( target_dir + \"/cases\" )\n",
    "\n",
    "    # Clear all existing raw summaries\n",
    "    delete_dir_contents( target_dir + \"/summaries\" )\n",
    "    \n",
    "    for filename in get_files_in_path( source_dir ) :\n",
    "    \n",
    "        # Get the file extension\n",
    "        extension = get_file_extension( filename )\n",
    "\n",
    "        if is_supported_file_type( filename ) :\n",
    "\n",
    "            # Load the file contents\n",
    "            contents = load_text_from_file( filename )\n",
    "\n",
    "            # Generate a new filename based on a numerical sequence\n",
    "            new_filename = generate_next_filename( target_dir + \"/\" + ( \"summaries\" if \"1.0\" in filename else \"cases\" ), \".txt\" )\n",
    "\n",
    "            # Write the new filename\n",
    "            write_data_to_file( new_filename, contents )\n",
    "    \n",
    "    \n",
    "def tokenize_file_contents( filename ) :\n",
    "    \n",
    "    # Load the stop words\n",
    "    stop_words = corpus.stopwords.words('english')\n",
    "    \n",
    "    # Load the file contents\n",
    "    contents = load_text_from_file( filename )\n",
    "    \n",
    "    # Define the filename tokens\n",
    "    content_tokens = []\n",
    "    \n",
    "    # Tokenize the sentences from the file contents\n",
    "    sentence_tokens = sent_tokenize( contents )\n",
    "    \n",
    "    for sentence in sentence_tokens :\n",
    "        \n",
    "        # Tokenize the words from the sentence\n",
    "        word_tokens = word_tokenize( sentence )\n",
    "        \n",
    "        # Remove the punctuation and convert the words to lowercase\n",
    "        words = [ word.lower() for word in word_tokens if word.isalpha() and dictionary.check( word ) ]\n",
    "        \n",
    "        # Remove the stop words\n",
    "        words = [ word for word in words if not word in stop_words ]\n",
    "        \n",
    "        # Append the sentence and tokens to the list\n",
    "        content_tokens.append( { 'sentence': sentence, 'words': words } )\n",
    "        \n",
    "    return content_tokens\n",
    "\n",
    "\n",
    "def load_files_in_path( path ) :\n",
    "    \n",
    "    # Create a list of tokens\n",
    "    contents = []\n",
    "    \n",
    "    for filename in get_files_in_path( path ) :\n",
    "        \n",
    "        # Append a dictionary to the contents\n",
    "        contents.append( load_text_from_file( filename ) )\n",
    "        \n",
    "    return contents\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files_in_path( path ) :\n",
    "    \n",
    "    # Create a list of tokens\n",
    "    tokens = []\n",
    "    \n",
    "    for filename in get_files_in_path( path ) :\n",
    "        \n",
    "        # Append a dictionary to the tokens\n",
    "        tokens.append( { \"filename\": get_file_basename( filename ), \"tokens\": tokenize_file_contents( filename ) } )\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_word2vec_model( entries ) :\n",
    "    \n",
    "    # Create documents list\n",
    "    documents = []\n",
    "    \n",
    "    for entry in entries :\n",
    "        \n",
    "        # Create a file token\n",
    "        file_tokens = []\n",
    "        \n",
    "        for sentence in entry[ \"tokens\" ] :\n",
    "            \n",
    "            # Add the word tokens in the document\n",
    "            file_tokens =  file_tokens + sentence[ \"words\" ]\n",
    "            \n",
    "        # Add the file tokens to the document\n",
    "        documents.append( file_tokens )\n",
    "    \n",
    "    model = gensim.models.Word2Vec ( documents, size = 50, window = 5, min_count = 1, workers = 100 )\n",
    "    \n",
    "    model.train( documents, total_examples = len( documents ), epochs = 10 )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_synonyms( model, word, min_similarity = 0.85 ) :\n",
    "    \n",
    "    # Define the similar words\n",
    "    synonyms = []\n",
    "    \n",
    "    # Get related word2vec words\n",
    "    suggested_words = model.wv.most_similar( positive = word, topn = 5 )\n",
    "    \n",
    "    # Limit words to a given similarity\n",
    "    for word in suggested_words :\n",
    "        \n",
    "        if word[ 1 ] > min_similarity :\n",
    "        \n",
    "            # Add word to \n",
    "            synonyms.append( word[ 0 ] )\n",
    "            \n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def get_tfidf( file_entries ) :\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    for file_index, file_entry in enumerate( file_entries ) :\n",
    "        \n",
    "        document = []\n",
    "        \n",
    "        for token_index, token in enumerate( file_entry[ 'tokens' ] ) :\n",
    "            \n",
    "            document.append( token[ 'sentence' ] )\n",
    "            \n",
    "        corpus.append( \" \".join( document ) )\n",
    "    \n",
    "    tf = TfidfVectorizer( smooth_idf = False, sublinear_tf = False, norm = None, analyzer = 'word' )\n",
    "    \n",
    "    return dict( zip( tfidf[ 1 ].get_feature_names(), tfidf[ 0 ].idf_ ) )\n",
    "\n",
    "\n",
    "def generate_score( weight, sentence_position, total_sentences, sentence_length, sentence_length_mean, sentence_length_std ) :\n",
    "    \n",
    "    position_factor = math.sin( math.radians( ( float( sentence_position ) / total_sentences ) ) )\n",
    "    \n",
    "    length_factor = sentence_length_mean / np.square( sentence_length  - ( sentence_length_std * 2 ) )\n",
    "\n",
    "    return position_factor * length_factor * weight\n",
    "\n",
    "\n",
    "def score_tokenized_sentences( file_entries, word2vec_model ):\n",
    "    \n",
    "    tfidf = get_tfidf( file_entries )\n",
    "    \n",
    "    # Iterate through the files\n",
    "    for file_index, file_entry in enumerate( file_entries ) :\n",
    "        \n",
    "        #if file_index < 10 :\n",
    "\n",
    "            # Define the cleaned sentences\n",
    "            cleaned_sentences = []\n",
    "            sentences_lengths = []\n",
    "\n",
    "            # Iterate through the tokens\n",
    "            for token_index, token in enumerate( file_entry['tokens' ] ):\n",
    "\n",
    "                # Add the words to the cleaned sentences\n",
    "                cleaned_sentences.append( \" \".join( str( word ) for word in token['words'] ) )\n",
    "                sentences_lengths.append( len( token[ 'words' ] ) )\n",
    "\n",
    "            for token_index, cleaned_sentence in enumerate( cleaned_sentences ) :\n",
    "                \n",
    "                # Words\n",
    "                words = file_entries[ file_index ][ 'tokens' ][ token_index ][ 'words' ]\n",
    "\n",
    "                # Define the sentence score\n",
    "                sentence_score = 0\n",
    "\n",
    "                # Iterate through the bag of words\n",
    "                for word_index, word in enumerate( words ) :\n",
    "\n",
    "                    if tfidf.has_key( word ) :\n",
    "                        \n",
    "                        # Increment the sentence score by the score\n",
    "                        sentence_score = sentence_score + tfidf[ word ]\n",
    "\n",
    "                # Get the sentence tags\n",
    "                pos_tags = pos_tag( file_entries[ file_index ][ 'tokens' ][ token_index ][ 'words' ] )\n",
    "\n",
    "                # Define the tags\n",
    "                tags = []\n",
    "\n",
    "                for tag in enumerate( pos_tags ) :\n",
    "\n",
    "                    tags.append( tag[ 1 ][ 1 ] )\n",
    "\n",
    "                weight = 0 if len( words ) < 1 else float( sentence_score ) / len( words )\n",
    "                total_sentences = len( cleaned_sentences )\n",
    "                sentence_position = token_index + 1\n",
    "                sentence_length = len( file_entries[ file_index ][ 'tokens' ][ token_index ][ 'words' ] )\n",
    "                sentence_length_mean = np.mean( sentences_lengths )\n",
    "                sentence_length_std = np.std( sentences_lengths )\n",
    "\n",
    "                # Create the store dictionary\n",
    "                score = { \n",
    "                    'weight': weight, \n",
    "                    'total_sentences': total_sentences, \n",
    "                    'sentence_position': sentence_position, \n",
    "                    'tense': 'past' if 'VBD' in tags else 'present' if 'VBG' in tags else 'uncategorized',\n",
    "                    'sentence': file_entries[ file_index ][ 'tokens' ][ token_index ][ 'sentence' ],\n",
    "                    'rating': generate_score( weight, sentence_position, total_sentences, sentence_length, sentence_length_mean, sentence_length_std ),\n",
    "                    'sentence_length': sentence_length\n",
    "                }\n",
    "\n",
    "                file_entries[ file_index ][ \"tokens\" ][ token_index ][ \"score\" ] = score\n",
    "                \n",
    "\n",
    "    return file_entries\n",
    "\n",
    "\n",
    "def get_tokens_minimum_ratings( scored_tokens, number_of_sentences = 5 ) :\n",
    "    \n",
    "    token_ratings = []\n",
    "    \n",
    "    for entry_index, scored_token in enumerate( scored_tokens ) :\n",
    "        \n",
    "        ratings = []\n",
    "        \n",
    "        for sentence_index, sentence in enumerate( scored_token[ \"tokens\" ] ) :\n",
    "            \n",
    "            if sentence.has_key( \"score\" ) :\n",
    "            \n",
    "                ratings.append( sentence[ \"score\" ][ \"rating\" ] )\n",
    "                \n",
    "            else :\n",
    "                \n",
    "                ratings.append( 0.0 )\n",
    "            \n",
    "        total_ratings = len( ratings )\n",
    "        \n",
    "        index = ( number_of_sentences if number_of_sentences < total_ratings else total_ratings ) - 1\n",
    "        \n",
    "        token_ratings.append( sorted( ratings, reverse = True )[ index ] )\n",
    "        \n",
    "    return token_ratings\n",
    "\n",
    "\n",
    "def create_summaries( entries, number_of_sentences ) :\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    # Declare the output directory\n",
    "    #output_dir = \"output\"\n",
    "    \n",
    "    # Clear all existing outputs\n",
    "    #delete_dir_contents( output_dir )\n",
    "    \n",
    "    # Get minimum ratings\n",
    "    token_minimum_ratings = get_tokens_minimum_ratings( entries, number_of_sentences )\n",
    "    \n",
    "    # Iterate through the tokens\n",
    "    for entry_index, entry in enumerate( entries ) :\n",
    "        \n",
    "        #if entry_index < 10 :\n",
    "\n",
    "            # Declare the summary sentences list\n",
    "            #sentences = [ entry[ \"tokens\" ][ 0 ][ \"score\" ][ \"sentence\" ] ] if len( entry[ \"tokens\" ] ) > 0 else [] \n",
    "            sentences = []\n",
    "\n",
    "            # Iterate through the tokens\n",
    "            for token_index, token in enumerate( entry[ \"tokens\" ] ) :\n",
    "\n",
    "                # Only use sentences that score above the minimum token\n",
    "                \n",
    "                if token.has_key( \"score\" ) :\n",
    "                    \n",
    "                    if token[ \"score\" ][ \"rating\" ] >= token_minimum_ratings[ entry_index ] :\n",
    "\n",
    "                        #Add the sentence to the sentences\n",
    "                        sentences.append( token[ \"score\" ][ \"sentence\"] )\n",
    "\n",
    "            # Generate a new filename based on a numerical sequence\n",
    "            #new_filename = generate_next_filename( output_dir, \".txt\" )\n",
    "\n",
    "            # Write the new filename\n",
    "            # write_data_to_file( new_filename, \" \".join( sentences ) )\n",
    "\n",
    "            summaries.append( \" \".join( sentences ) )\n",
    "            \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the raw files\n",
    "generate_raw_files( \"./source\", \"./target\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cases tokens\n",
    "tokenized_cases = tokenize_files_in_path( \"./target/cases\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries tokens\n",
    "tokenized_summaries = tokenize_files_in_path( \"./target/summaries\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cases word2vec model\n",
    "cases_word2vec_model = create_word2vec_model( tokenized_cases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries word2vec model\n",
    "summaries_word2vec_model = create_word2vec_model( tokenized_summaries )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate general (cases + summaries ) word2vec model\n",
    "general_word2vec_model = create_word2vec_model( tokenized_cases + tokenized_summaries )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test word2vec models (check similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manslaughter', 'convicted', 'provocation', 'rape', 'kidnapping']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the cases model\n",
    "print get_synonyms( cases_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['violence', 'robbery', 'count', 'kidnapping', 'intent']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the summaries model\n",
    "print get_synonyms( summaries_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kidnapping', 'manslaughter']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the general model\n",
    "print get_synonyms( general_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the cases\n",
    "scored_case_tokens = score_tokenized_sentences( tokenized_cases, general_word2vec_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the summaries\n",
    "scored_summary_tokens = score_tokenized_sentences( tokenized_summaries, general_word2vec_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_summaries = create_summaries( scored_case_tokens, number_of_sentences = 15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_summaries = create_summaries( scored_summary_tokens, number_of_sentences = 15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The  defendant then suspected the plaintiff and the two key holders  in  the  loss  of  the said money. He stated that he was  involved  in  the  termination  exercise since he was the Chairman Staff Commendation and Disciplinary Committee. The position of the law in  regard  to  termination  of  any  employee  from employment with or without notice is now beyond any doubt. But this was a matter  which  was  subject  of  a disciplinary proceedings where it was established  that  the  plaintiff  was not connected with the loss of the said dollars. The plaintiff has emphatic that after handing over the said dollars  to  the two key holders, he had nothing to do with its  loss. It was done without  considering  the number of good years the plaintiff  had  put  in  the  defendant's  service. The plaintiff was  one  of  the suspects because he was the one who had brought the money  which  eventually got stolen. It  was connected to loss  of  public  money:   see  Abraham  Waligo   Vs   Attorney General HCCS 533/94 (unreported). The plaintiff contended  that  his  arrest,  detention  and  dismissal  were arbitrary, outrageous and high handed. There was therefore no evidence  to  show  that  the plaintiff was treated in oppressive, arbitrary or  unconstitutional  manner.\",\n",
       " 'In its various sections, (Chapter 2) in Part II thereof, sets out forms of Acts of Parliament and Bills. We understand subsection (2) to imply that a law is as good as a dead law until the day upon which it becomes enforceable. These provisions re inforce the view that an Act becomes operational either on a date specified by the Act itself or upon notification in the Gazette. Mr Matsikos contention that the Constitutional Court erred either in law or in fact in so holding has no foundation whatsoever. Article 91(1) of the Constitution and sections 9(2), 19(2) and 20(1) of (Chapter 2) upon which he relied on do not support any of his arguments that the petition was filed out of time. There was therefore no need for the respondents to plead circumstances of exemption as required by Order 7, rule 6 of Civil Procedure Rule. The provisions of (Chapter 2) to which we have referred, more especially sections 13 and 14, are clear on the purpose of publication and on the date of commencement of an Act of Parliament. The purpose of publication is to let everybody be aware of the terms of the Act, its number and the date of presidential assent (section 13(2)). So where did the Court err either in law or in fact when it concluded that the petitioners should have perceived the alleged breach of the Constitution by 17 July 2002? We direct that the hearing of the petition in the Constitutional Court proceeds expeditiously as required by rule 10 of Legal Notice number 4 of 1996.']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_summaries[ 0 : 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE REPUBLIC OF UGANDA IN THE HIGH COURT OF UGANDA AT KAMPALA CIVIL SUIT No. 669 OF 2001 FACTS  The brief facts giving rise to the cause of action are that the plaintiff was appointed in the employment of the defendant on the 15th July 1979. He served in various departments of the bank until 21st September 1999 when at the instance of the bank he was arrested and detained at Kawempe police station. He was arrested because of the loss of some US $200.000 which had been collected form Kasese branch and handed to two officers of the bank. He was released on bond on 27th September 1999 but kept on reporting at a police post at the defendants head office premises in Kampala. He appeared before the Staff Commendation and Disciplinary Committee between January and March 2000 on allegation of the loss of the said money. The committee exonerated him but was however terminated on the 12th June 2000. The other two suspects were however dismissed from service. The plaintiff appealed to the defendant against the termination but he was turned down. ISSUE 1. Whether the plaintiffs dismissal was lawful and/or justified. Whether the arrest and detention of the plaintiff at the instance of the defendant of the defendant was legal and/or justifiable. Whether the plaintiff is entitled to the claim for exemplary/punitive damages. What remedies if any, are available to the plaintiff. HELD Termination was Unlawful Arrest was Lawful Exemplary Damages were denied Damages and terminal benefits awarded',\n",
       " 'ATTORNEY-GENERAL V RWANYARARE AND OTHERS  Division: Supreme Court of Uganda at Mengo  Date of Judgment: 21 April 2004  Case Number: 2/03  Before: Odoki CJ, Oder, Tsekooko, Karokora, Mulenga, Kanyeihamba  and Kato JJSC  Sourced by: LawAfrica  Summarised by: M Kibanga  [1] Statute law  Act of Parliament  When an Act of Parliament comes into force  When an Act of  Parliament breaches the Constitution  What determines the date of commencement of a statute. Editors Summary  The Parliament of Uganda enacted the Political Parties and Organizations Act and the President assented it on 2 May 2002. It was gazetted on 17 July 2002. The respondents felt aggrieved by the Act and filed a constitutional petition seeking inter alia a declaration that it was inconsistent with the Constitution. The Attorney-General filed an answer but raised a point of law as regards the competence of the petition, arguing that the petition had been filed out of time. The Constitutional Court dismissed the objection on the ground that the petition was filed on 31 July 2002 and was within time, hence competent, the attorney-General then appealed to the Supreme Court. Held  An act becomes operational either on a date specified by the Act itself or upon notification in the  Uganda Gazette. In the present case, the date of commencement is different from the date of assent. An  Act that has not come into force cannot violate the Constitution. The petition was filed within time. Appeal was dismissed. No cases']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_summaries[ 0 : 2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = load_files_in_path( './target/summaries' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec(vector_size=250, window=2, min_count=5, workers=11,alpha=0.025, min_alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(case_summaries[0:250])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"models/cases.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = word_tokenize(summaries[ 3 ].lower())\n",
    "v1 = model.infer_vector(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('114', 0.7531960606575012),\n",
       " ('24', 0.6740461587905884),\n",
       " ('193', 0.6674911975860596),\n",
       " ('29', 0.664217472076416),\n",
       " ('66', 0.6601599454879761),\n",
       " ('95', 0.6589336395263672),\n",
       " ('227', 0.6533979177474976),\n",
       " ('237', 0.6516009569168091),\n",
       " ('245', 0.6460717916488647),\n",
       " ('201', 0.6456685066223145)]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar([v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
