{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legal Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import textract\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import corpus, pos_tag \n",
    "import gensim \n",
    "import enchant\n",
    "import re\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import logging\n",
    "import pprint\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter( indent = 4, width = 150 )\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the english dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = enchant.Dict( \"en_US\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all required methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_path( path ) :\n",
    "    \n",
    "    for root, dirs, files in os.walk( path ) :\n",
    "        \n",
    "        for filename in files :\n",
    "            \n",
    "            yield root + \"/\" + filename\n",
    "            \n",
    "            \n",
    "def load_text_from_file( filename ) :\n",
    "    \n",
    "    contents = textract.process( filename, encoding='ascii' )\n",
    "    contents = re.sub( '\\\\n',' ', str( contents ) )\n",
    "    \n",
    "    return contents\n",
    "\n",
    "\n",
    "def delete_dir_contents( path ) :\n",
    "    \n",
    "    shutil.rmtree( path )\n",
    "    \n",
    "    os.makedirs( path )\n",
    "    \n",
    "    \n",
    "def number_of_files_in_path( path ) :\n",
    "    \n",
    "    return len( next( os.walk( path ) )[ 2 ] )\n",
    "\n",
    "\n",
    "def get_file_extension( filename ) :\n",
    "    \n",
    "    return os.path.splitext( filename )[ 1 ]\n",
    "\n",
    "\n",
    "def get_file_basename( filename ) :\n",
    "    \n",
    "    return os.path.basename( filename )\n",
    "\n",
    "\n",
    "def is_supported_file_type( filename ) :\n",
    "    \n",
    "    # A list of all supported extensions\n",
    "    valid_extensions = [ \".doc\", \".docx\", \".pdf\", \".rtf\" ]\n",
    "    \n",
    "    return valid_extensions.count( get_file_extension( filename ) ) == 1\n",
    "\n",
    "\n",
    "def generate_next_filename( path, extension ) :\n",
    "    \n",
    "    return path + \"/\" + str( number_of_files_in_path( path ) + 1 ) + extension\n",
    "\n",
    "\n",
    "def write_data_to_file( filename, data ) :\n",
    "    \n",
    "    # Open the file for writing\n",
    "    file = open( filename, 'w' )\n",
    "    \n",
    "    # Write the data to the file\n",
    "    file.write( data )\n",
    "    \n",
    "    # Close the file\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def generate_raw_files( source_dir, target_dir ) :\n",
    "    \n",
    "    # Clear all existing raw cases\n",
    "    delete_dir_contents( target_dir + \"/cases\" )\n",
    "\n",
    "    # Clear all existing raw summaries\n",
    "    delete_dir_contents( target_dir + \"/summaries\" )\n",
    "    \n",
    "    for filename in get_files_in_path( source_dir ) :\n",
    "    \n",
    "        # Get the file extension\n",
    "        extension = get_file_extension( filename )\n",
    "\n",
    "        if is_supported_file_type( filename ) :\n",
    "\n",
    "            # Load the file contents\n",
    "            contents = load_text_from_file( filename )\n",
    "\n",
    "            # Generate a new filename based on a numerical sequence\n",
    "            new_filename = generate_next_filename( target_dir + \"/\" + ( \"summaries\" if \"1.0\" in filename else \"cases\" ), \".txt\" )\n",
    "\n",
    "            # Write the new filename\n",
    "            write_data_to_file( new_filename, contents )\n",
    "    \n",
    "    \n",
    "def tokenize_file_contents( filename ) :\n",
    "    \n",
    "    # Load the stop words\n",
    "    stop_words = corpus.stopwords.words('english')\n",
    "    \n",
    "    # Load the file contents\n",
    "    contents = load_text_from_file( filename )\n",
    "    \n",
    "    # Define the filename tokens\n",
    "    content_tokens = []\n",
    "    \n",
    "    # Tokenize the sentences from the file contents\n",
    "    sentence_tokens = sent_tokenize( contents )\n",
    "    \n",
    "    for sentence in sentence_tokens :\n",
    "        \n",
    "        # Tokenize the words from the sentence\n",
    "        word_tokens = word_tokenize( sentence )\n",
    "        \n",
    "        # Remove the punctuation and convert the words to lowercase\n",
    "        words = [ word.lower() for word in word_tokens if word.isalpha() and dictionary.check( word ) ]\n",
    "        \n",
    "        # Remove the stop words\n",
    "        words = [ word for word in words if not word in stop_words ]\n",
    "        \n",
    "        # Append the sentence and tokens to the list\n",
    "        content_tokens.append( { 'sentence': sentence, 'words': words } )\n",
    "        \n",
    "    return content_tokens\n",
    "\n",
    "\n",
    "def load_files_in_path( path ) :\n",
    "    \n",
    "    # Create a list of tokens\n",
    "    contents = []\n",
    "    \n",
    "    for filename in get_files_in_path( path ) :\n",
    "        \n",
    "        # Append a dictionary to the contents\n",
    "        contents.append( load_text_from_file( filename ) )\n",
    "        \n",
    "    return contents\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files_in_path( path ) :\n",
    "    \n",
    "    # Create a list of tokens\n",
    "    tokens = []\n",
    "    \n",
    "    for filename in get_files_in_path( path ) :\n",
    "        \n",
    "        # Append a dictionary to the tokens\n",
    "        tokens.append( { \"filename\": get_file_basename( filename ), \"tokens\": tokenize_file_contents( filename ) } )\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_word2vec_model( entries ) :\n",
    "    \n",
    "    # Create documents list\n",
    "    documents = []\n",
    "    \n",
    "    for entry in entries :\n",
    "        \n",
    "        # Create a file token\n",
    "        file_tokens = []\n",
    "        \n",
    "        for sentence in entry[ \"tokens\" ] :\n",
    "            \n",
    "            # Add the word tokens in the document\n",
    "            file_tokens =  file_tokens + sentence[ \"words\" ]\n",
    "            \n",
    "        # Add the file tokens to the document\n",
    "        documents.append( file_tokens )\n",
    "    \n",
    "    model = gensim.models.Word2Vec ( documents, size = 50, window = 5, min_count = 1, workers = 100 )\n",
    "    \n",
    "    model.train( documents, total_examples = len( documents ), epochs = 10 )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_synonyms( model, word, min_similarity = 0.85 ) :\n",
    "    \n",
    "    # Define the similar words\n",
    "    synonyms = []\n",
    "    \n",
    "    # Get related word2vec words\n",
    "    suggested_words = model.wv.most_similar( positive = word, topn = 5 )\n",
    "    \n",
    "    # Limit words to a given similarity\n",
    "    for word in suggested_words :\n",
    "        \n",
    "        if word[ 1 ] > min_similarity :\n",
    "        \n",
    "            # Add word to \n",
    "            synonyms.append( word[ 0 ] )\n",
    "            \n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def get_tfidf( file_entries ) :\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    for file_index, file_entry in enumerate( file_entries ) :\n",
    "        \n",
    "        document = []\n",
    "        \n",
    "        for token_index, token in enumerate( file_entry[ 'tokens' ] ) :\n",
    "            \n",
    "            document.append( token[ 'sentence' ] )\n",
    "            \n",
    "        corpus.append( \" \".join( document ) )\n",
    "    \n",
    "    tf = TfidfVectorizer( smooth_idf = False, sublinear_tf = False, norm = None, analyzer = 'word' )\n",
    "    \n",
    "    tfidf = ( tf, tf.fit( corpus ) )\n",
    "    \n",
    "    return dict( zip( tfidf[ 1 ].get_feature_names(), tfidf[ 0 ].idf_ ) )\n",
    "\n",
    "\n",
    "def generate_score( weight, sentence_position, total_sentences, sentence_length, sentence_length_mean, sentence_length_std ) :\n",
    "    \n",
    "    position_factor = math.sin( math.radians( ( float( sentence_position ) / total_sentences ) * 90 ) )\n",
    "    \n",
    "    length_factor = sentence_length_mean / np.square( sentence_length  - ( sentence_length_std * 2 ) )\n",
    "\n",
    "    return position_factor * length_factor * weight\n",
    "\n",
    "\n",
    "def score_tokenized_sentences( file_entries, word2vec_model ):\n",
    "    \n",
    "    tfidf = get_tfidf( file_entries )\n",
    "    \n",
    "    # Iterate through the files\n",
    "    for file_index, file_entry in enumerate( file_entries ) :\n",
    "        \n",
    "        #if file_index < 10 :\n",
    "\n",
    "            # Define the cleaned sentences\n",
    "            cleaned_sentences = []\n",
    "            sentences_lengths = []\n",
    "\n",
    "            # Iterate through the tokens\n",
    "            for token_index, token in enumerate( file_entry['tokens' ] ):\n",
    "\n",
    "                # Add the words to the cleaned sentences\n",
    "                cleaned_sentences.append( \" \".join( str( word ) for word in token['words'] ) )\n",
    "                sentences_lengths.append( len( token[ 'words' ] ) )\n",
    "\n",
    "            for token_index, cleaned_sentence in enumerate( cleaned_sentences ) :\n",
    "                \n",
    "                # Words\n",
    "                words = file_entries[ file_index ][ 'tokens' ][ token_index ][ 'words' ]\n",
    "\n",
    "                # Define the sentence score\n",
    "                sentence_score = 0\n",
    "\n",
    "                # Iterate through the bag of words\n",
    "                for word_index, word in enumerate( words ) :\n",
    "\n",
    "                    if tfidf.has_key( word ) :\n",
    "                        \n",
    "                        # Increment the sentence score by the score\n",
    "                        sentence_score = sentence_score + tfidf[ word ]\n",
    "\n",
    "                # Get the sentence tags\n",
    "                pos_tags = pos_tag( file_entries[ file_index ][ 'tokens' ][ token_index ][ 'words' ] )\n",
    "\n",
    "                # Define the tags\n",
    "                tags = []\n",
    "\n",
    "                for tag in enumerate( pos_tags ) :\n",
    "\n",
    "                    tags.append( tag[ 1 ][ 1 ] )\n",
    "\n",
    "                weight = 0 if len( words ) < 1 else float( sentence_score ) / len( words )\n",
    "                total_sentences = len( cleaned_sentences )\n",
    "                sentence_position = token_index + 1\n",
    "                sentence_length = len( file_entries[ file_index ][ 'tokens' ][ token_index ][ 'words' ] )\n",
    "                sentence_length_mean = np.mean( sentences_lengths )\n",
    "                sentence_length_std = np.std( sentences_lengths )\n",
    "\n",
    "                # Create the store dictionary\n",
    "                score = { \n",
    "                    'weight': weight, \n",
    "                    'total_sentences': total_sentences, \n",
    "                    'sentence_position': sentence_position, \n",
    "                    'tense': 'past' if 'VBD' in tags else 'present' if 'VBG' in tags else 'uncategorized',\n",
    "                    'sentence': file_entries[ file_index ][ 'tokens' ][ token_index ][ 'sentence' ],\n",
    "                    'rating': generate_score( weight, sentence_position, total_sentences, sentence_length, sentence_length_mean, sentence_length_std ),\n",
    "                    'sentence_length': sentence_length\n",
    "                }\n",
    "\n",
    "                file_entries[ file_index ][ \"tokens\" ][ token_index ][ \"score\" ] = score\n",
    "                \n",
    "\n",
    "    return file_entries\n",
    "\n",
    "\n",
    "def get_tokens_minimum_ratings( scored_tokens, number_of_sentences = 5 ) :\n",
    "    \n",
    "    token_ratings = []\n",
    "    \n",
    "    for entry_index, scored_token in enumerate( scored_tokens ) :\n",
    "        \n",
    "        ratings = []\n",
    "        \n",
    "        for sentence_index, sentence in enumerate( scored_token[ \"tokens\" ] ) :\n",
    "            \n",
    "            if sentence.has_key( \"score\" ) :\n",
    "            \n",
    "                ratings.append( sentence[ \"score\" ][ \"rating\" ] )\n",
    "                \n",
    "            else :\n",
    "                \n",
    "                ratings.append( 0.0 )\n",
    "            \n",
    "        total_ratings = len( ratings )\n",
    "        \n",
    "        index = ( number_of_sentences if number_of_sentences < total_ratings else total_ratings ) - 1\n",
    "        \n",
    "        token_ratings.append( sorted( ratings, reverse = True )[ index ] )\n",
    "        \n",
    "    return token_ratings\n",
    "\n",
    "\n",
    "def create_summaries( entries, number_of_sentences ) :\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    # Declare the output directory\n",
    "    #output_dir = \"output\"\n",
    "    \n",
    "    # Clear all existing outputs\n",
    "    #delete_dir_contents( output_dir )\n",
    "    \n",
    "    # Get minimum ratings\n",
    "    token_minimum_ratings = get_tokens_minimum_ratings( entries, number_of_sentences )\n",
    "    \n",
    "    # Iterate through the tokens\n",
    "    for entry_index, entry in enumerate( entries ) :\n",
    "        \n",
    "        #if entry_index < 10 :\n",
    "\n",
    "            # Declare the summary sentences list\n",
    "            #sentences = [ entry[ \"tokens\" ][ 0 ][ \"score\" ][ \"sentence\" ] ] if len( entry[ \"tokens\" ] ) > 0 else [] \n",
    "            sentences = []\n",
    "\n",
    "            # Iterate through the tokens\n",
    "            for token_index, token in enumerate( entry[ \"tokens\" ] ) :\n",
    "\n",
    "                # Only use sentences that score above the minimum token\n",
    "                \n",
    "                if token.has_key( \"score\" ) :\n",
    "                    \n",
    "                    if token[ \"score\" ][ \"rating\" ] >= token_minimum_ratings[ entry_index ] :\n",
    "\n",
    "                        #Add the sentence to the sentences\n",
    "                        sentences.append( token[ \"score\" ][ \"sentence\"] )\n",
    "\n",
    "            # Generate a new filename based on a numerical sequence\n",
    "            #new_filename = generate_next_filename( output_dir, \".txt\" )\n",
    "\n",
    "            # Write the new filename\n",
    "            # write_data_to_file( new_filename, \" \".join( sentences ) )\n",
    "\n",
    "            summaries.append( \" \".join( sentences ) )\n",
    "            \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the raw files\n",
    "generate_raw_files( \"./source\", \"./target\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cases tokens\n",
    "tokenized_cases = tokenize_files_in_path( \"./target/cases\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries tokens\n",
    "tokenized_summaries = tokenize_files_in_path( \"./target/summaries\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cases word2vec model\n",
    "cases_word2vec_model = create_word2vec_model( tokenized_cases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries word2vec model\n",
    "summaries_word2vec_model = create_word2vec_model( tokenized_summaries )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate general (cases + summaries ) word2vec model\n",
    "general_word2vec_model = create_word2vec_model( tokenized_cases + tokenized_summaries )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test word2vec models (check similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manslaughter', 'kidnapping', 'convicted', 'audiotapes', 'defilement']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the cases model\n",
    "print get_synonyms( cases_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robbery', 'counts', 'violence', 'implying', 'convicted']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the summaries model\n",
    "print get_synonyms( summaries_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manslaughter', 'incarcerated', 'kidnapping']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the general model\n",
    "print get_synonyms( general_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the cases\n",
    "scored_case_tokens = score_tokenized_sentences( tokenized_cases, general_word2vec_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the summaries\n",
    "scored_summary_tokens = score_tokenized_sentences( tokenized_summaries, general_word2vec_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_summaries = create_summaries( scored_case_tokens, number_of_sentences = 15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_summaries = create_summaries( scored_summary_tokens, number_of_sentences = 15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Soon after that appointment,  he  went  for  a  course  which resulted in another appointment in April 1981 as a  clerk/machine  operator. He  worked  in  the above Division from August 1996 up-to December 1999 when he was  transferred to the Real Estate Department of  the  same  bank. So he reported on duty on Tuesday  where  he  learnt from the key holders that the money he had given them had  got  stolen  from the safe. He also suffered embarrassment at the hand of his  neighbours who were present during the house search because he was  believed  to  be  a honest person. After that he continued working in Real Estates until 12th  June  2000 when he received a letter terminating his  services  (exhibit  P10). The  defendant then suspected the plaintiff and the two key holders  in  the  loss  of  the said money. He stated that he was  involved  in  the  termination  exercise since he was the Chairman Staff Commendation and Disciplinary Committee. The position of the law in  regard  to  termination  of  any  employee  from employment with or without notice is now beyond any doubt. But this was a matter  which  was  subject  of  a disciplinary proceedings where it was established  that  the  plaintiff  was not connected with the loss of the said dollars. The plaintiff has emphatic that after handing over the said dollars  to  the two key holders, he had nothing to do with its  loss. It was done without  considering  the number of good years the plaintiff  had  put  in  the  defendant's  service. The plaintiff was  one  of  the suspects because he was the one who had brought the money  which  eventually got stolen. It  was connected to loss  of  public  money:   see  Abraham  Waligo   Vs   Attorney General HCCS 533/94 (unreported). The plaintiff contended  that  his  arrest,  detention  and  dismissal  were arbitrary, outrageous and high handed. There was therefore no evidence  to  show  that  the plaintiff was treated in oppressive, arbitrary or  unconstitutional  manner.\",\n",
       " 'The respondents filed a notice of grounds of affirming the decision of the Constitutional Court pursuant to rule 87 of the Rules of this Court. The three grounds of appeal revolve around one and the same point namely whether or not the petition was filed in time. He argued that therefore by filing the petition on 31 July 2002, the respondent lodged it in court within the prescribed time. In its various sections, (Chapter 2) in Part II thereof, sets out forms of Acts of Parliament and Bills. We understand subsection (2) to imply that a law is as good as a dead law until the day upon which it becomes enforceable. These provisions re inforce the view that an Act becomes operational either on a date specified by the Act itself or upon notification in the Gazette. Mr Matsikos contention that the Constitutional Court erred either in law or in fact in so holding has no foundation whatsoever. Article 91(1) of the Constitution and sections 9(2), 19(2) and 20(1) of (Chapter 2) upon which he relied on do not support any of his arguments that the petition was filed out of time. There was therefore no need for the respondents to plead circumstances of exemption as required by Order 7, rule 6 of Civil Procedure Rule. Mr Matsiko referred us to the conclusions of the ruling of the Constitutional Court and contended that the Court speculated. The description of this passage by Mr Matsiko as speculative is with respect, wholly wrong and without any basis whatsoever. The provisions of (Chapter 2) to which we have referred, more especially sections 13 and 14, are clear on the purpose of publication and on the date of commencement of an Act of Parliament. The purpose of publication is to let everybody be aware of the terms of the Act, its number and the date of presidential assent (section 13(2)). So where did the Court err either in law or in fact when it concluded that the petitioners should have perceived the alleged breach of the Constitution by 17 July 2002? We direct that the hearing of the petition in the Constitutional Court proceeds expeditiously as required by rule 10 of Legal Notice number 4 of 1996.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_summaries[ 0 : 2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = load_files_in_path( './target/summaries' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec( vector_size = 250, window = 2, min_count = 5, workers = 11, alpha = 0.025, min_alpha = 0.025 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [ TaggedDocument( words = word_tokenize( _d.lower() ), tags=[ str( i ) ] ) for i, _d in enumerate( case_summaries[0:250])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"models/cases.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = word_tokenize(summaries[ 2 ].lower())\n",
    "v1 = model.infer_vector(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('96', 0.7527245879173279),\n",
       " ('2', 0.7477607727050781),\n",
       " ('53', 0.6964675188064575),\n",
       " ('28', 0.6769248247146606),\n",
       " ('60', 0.6654722094535828),\n",
       " ('62', 0.6597374677658081),\n",
       " ('201', 0.6585685610771179),\n",
       " ('46', 0.6467065215110779),\n",
       " ('70', 0.6435794830322266),\n",
       " ('104', 0.6376673579216003)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar([v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
