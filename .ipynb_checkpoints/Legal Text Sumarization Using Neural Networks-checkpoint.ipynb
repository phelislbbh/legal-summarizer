{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legal Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import textract\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import corpus, pos_tag \n",
    "import gensim \n",
    "import enchant\n",
    "import re\n",
    "import math\n",
    "\n",
    "import logging\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter( indent = 4, width = 150 )\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define english dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all required methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_path( path ) :\n",
    "    \n",
    "    for root, dirs, files in os.walk( path ) :\n",
    "        \n",
    "        for filename in files :\n",
    "            \n",
    "            yield root + \"/\" + filename\n",
    "            \n",
    "            \n",
    "def load_text_from_file( filename ) :\n",
    "    \n",
    "    # Load the data\n",
    "    contents = textract.process( filename, encoding='ascii' )\n",
    "    \n",
    "    # Clean the data\n",
    "    contents = contents\n",
    "    #contents = re.sub( '\\[\\d+\\]', '', contents.strip() )\n",
    "    #contents = re.sub( '\\d+\\.', '', contents.strip() )\n",
    "    #contents = contents.replace(\"\\n\", \" .\")\n",
    "\n",
    "    return contents\n",
    "\n",
    "\n",
    "def delete_dir_contents( path ) :\n",
    "    \n",
    "    shutil.rmtree( path )\n",
    "    \n",
    "    os.makedirs( path )\n",
    "    \n",
    "    \n",
    "def number_of_files_in_path( path ) :\n",
    "    \n",
    "    return len( next( os.walk( path ) )[ 2 ] )\n",
    "\n",
    "\n",
    "def get_file_extension( filename ) :\n",
    "    \n",
    "    return os.path.splitext( filename )[ 1 ]\n",
    "\n",
    "\n",
    "def get_file_basename( filename ) :\n",
    "    \n",
    "    return os.path.basename( filename )\n",
    "\n",
    "\n",
    "def is_supported_file_type( filename ) :\n",
    "    \n",
    "    # A list of all supported extensions\n",
    "    valid_extensions = [ \".doc\", \".docx\", \".pdf\", \".rtf\" ]\n",
    "    \n",
    "    return valid_extensions.count( get_file_extension( filename ) ) == 1\n",
    "\n",
    "\n",
    "def generate_next_filename( path, extension ) :\n",
    "    \n",
    "    return path + \"/\" + str( number_of_files_in_path( path ) + 1 ) + extension\n",
    "\n",
    "\n",
    "def write_data_to_file( filename, data ) :\n",
    "    \n",
    "    # Open the file for writing\n",
    "    file = open( filename, 'w' )\n",
    "    \n",
    "    # Write the data to the file\n",
    "    file.write( data )\n",
    "    \n",
    "    # Close the file\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def generate_raw_files( source_dir, target_dir ) :\n",
    "    \n",
    "    # Clear all existing raw cases\n",
    "    delete_dir_contents( target_dir + \"/cases\" )\n",
    "\n",
    "    # Clear all existing raw summaries\n",
    "    delete_dir_contents( target_dir + \"/summaries\" )\n",
    "    \n",
    "    for filename in get_files_in_path( source_dir ) :\n",
    "    \n",
    "        # Get the file extension\n",
    "        extension = get_file_extension( filename )\n",
    "\n",
    "        if is_supported_file_type( filename ) :\n",
    "\n",
    "            # Load the file contents\n",
    "            contents = load_text_from_file( filename )\n",
    "\n",
    "            # Generate a new filename based on a numerical sequence\n",
    "            new_filename = generate_next_filename( target_dir + \"/\" + ( \"summaries\" if \"1.0\" in filename else \"cases\" ), \".txt\" )\n",
    "\n",
    "            # Write the new filename\n",
    "            write_data_to_file( new_filename, contents )\n",
    "    \n",
    "    \n",
    "def tokenize_file_contents( filename ) :\n",
    "    \n",
    "    # Load the stop words\n",
    "    stop_words = corpus.stopwords.words('english')\n",
    "    \n",
    "    # Load the file contents\n",
    "    contents = load_text_from_file( filename )\n",
    "    \n",
    "    # Define the filename tokens\n",
    "    content_tokens = []\n",
    "    \n",
    "    # Tokenize the sentences from the file contents\n",
    "    sentence_tokens = sent_tokenize( contents )\n",
    "    \n",
    "    for sentence in sentence_tokens :\n",
    "        \n",
    "        # Tokenize the words from the sentence\n",
    "        word_tokens = word_tokenize( sentence )\n",
    "        \n",
    "        # Remove the punctuation and convert the words to lowercase\n",
    "        words = [ word.lower() for word in word_tokens if word.isalpha() and dictionary.check( word ) ]\n",
    "        \n",
    "        # Remove the stop words\n",
    "        words = [ word for word in words if not word in stop_words ]\n",
    "        \n",
    "        # Append the sentence and tokens to the list\n",
    "        content_tokens.append( { 'sentence': sentence, 'words': words } )\n",
    "        \n",
    "    return content_tokens\n",
    "\n",
    "\n",
    "def tokenize_files_in_path( path ) :\n",
    "    \n",
    "    # Create a list of tokens\n",
    "    tokens = []\n",
    "    \n",
    "    for filename in get_files_in_path( path ) :\n",
    "        \n",
    "        # Append a dictionary to the tokens\n",
    "        tokens.append( { \"filename\": get_file_basename( filename ), \"tokens\": tokenize_file_contents( filename ) } )\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_word2vec_model( entries ) :\n",
    "    \n",
    "    # Create documents list\n",
    "    documents = []\n",
    "    \n",
    "    for entry in entries :\n",
    "        \n",
    "        # Create a file token\n",
    "        file_tokens = []\n",
    "        \n",
    "        for sentence in entry[ \"tokens\" ] :\n",
    "            \n",
    "            # Add the word tokens in the document\n",
    "            file_tokens =  file_tokens + sentence[ \"words\" ]\n",
    "            \n",
    "        # Add the file tokens to the document\n",
    "        documents.append( file_tokens )\n",
    "    \n",
    "    model = gensim.models.Word2Vec ( documents, size = 50, window = 5, min_count = 1, workers = 100 )\n",
    "    \n",
    "    model.train( documents, total_examples = len( documents ), epochs = 10 )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_synonyms( model, word, min_similarity = 0.85 ) :\n",
    "    \n",
    "    # Define the similar words\n",
    "    synonyms = []\n",
    "    \n",
    "    # Get related word2vec words\n",
    "    suggested_words = model.wv.most_similar( positive = word, topn = 5 )\n",
    "    \n",
    "    # Limit words to a given similarity\n",
    "    for word in suggested_words :\n",
    "        \n",
    "        if word[ 1 ] > min_similarity :\n",
    "        \n",
    "            # Add word to \n",
    "            synonyms.append( word[ 0 ] )\n",
    "            \n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def generate_score( weight, sentence_position, total_sentences ) :\n",
    "    \n",
    "    new_value = float( sentence_position ) / total_sentences\n",
    "    \n",
    "    return math.sin( math.radians( math.radians( new_value * 90 ) ) ) * weight\n",
    "\n",
    "\n",
    "def score_tokenized_sentences( file_entries ):\n",
    "    \n",
    "    # Iterate through the files\n",
    "    for file_index, file_entry in enumerate( file_entries ) :\n",
    "        \n",
    "        # Define the cleaned sentences\n",
    "        cleaned_sentences = []\n",
    "        \n",
    "        # Iterate through the tokens\n",
    "        for token_index, token in enumerate( file_entry['tokens' ] ):\n",
    "            \n",
    "            # Add the words to the cleaned sentences\n",
    "            cleaned_sentences.append( \" \".join( str( word ) for word in token['words'] ) )\n",
    "            \n",
    "        # Create a word vectorizer\n",
    "        vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \n",
    "\n",
    "        # Train the vectorizer\n",
    "        train_data_features = vectorizer.fit_transform( cleaned_sentences )\n",
    "    \n",
    "        for token_index, cleaned_sentence in enumerate( cleaned_sentences ) :\n",
    "            \n",
    "            # Create the bag of words\n",
    "            bag_of_words = vectorizer.transform( [ cleaned_sentence ] ).toarray()[ 0 ]\n",
    "            \n",
    "            # Define the sentence score\n",
    "            sentence_score = 0\n",
    "            \n",
    "            # Iterate through the bag of words\n",
    "            for score_index, score in enumerate( bag_of_words ) :\n",
    "                \n",
    "                # Increment the sentence score by the score\n",
    "                sentence_score = sentence_score + score\n",
    "                \n",
    "            # Get the sentence tags\n",
    "            pos_tags = pos_tag( file_entries[ file_index ][ 'tokens' ][ token_index ][ 'words' ] )\n",
    "            \n",
    "            # Define the tags\n",
    "            tags = []\n",
    "            \n",
    "            for tag in enumerate( pos_tags ) :\n",
    "                \n",
    "                tags.append( tag[ 1 ][ 1 ] )\n",
    "                \n",
    "            weight = float( sentence_score ) / len( bag_of_words )\n",
    "            total_sentences = len( cleaned_sentences )\n",
    "            sentence_position = token_index + 1\n",
    "                \n",
    "            # Create the store dictionary\n",
    "            score = { \n",
    "                'weight': weight, \n",
    "                'total_sentences': total_sentences, \n",
    "                'sentence_position': sentence_position, \n",
    "                'tense': 'past' if 'VBD' in tags else 'present' if 'VBG' in tags else 'uncategorized',\n",
    "                'sentence': file_entries[ file_index ][ 'tokens' ][ token_index ][ 'sentence' ],\n",
    "                'rating': generate_score( weight, sentence_position, total_sentences )\n",
    "            }\n",
    "        \n",
    "            file_entries[ file_index ][ \"tokens\" ][ token_index ][ \"score\" ] = score\n",
    "            \n",
    "    return file_entries\n",
    "\n",
    "\n",
    "def extract_ratings_from_scored_tokens( scored_tokens ) :\n",
    "    \n",
    "    ratings = []\n",
    "    \n",
    "    for entry_index, scored_token in enumerate( scored_tokens ) :\n",
    "        \n",
    "        for sentence_index, sentence in enumerate( scored_token[ \"tokens\" ] ) :\n",
    "            \n",
    "            ratings.append( sentence[ \"score\" ][ \"rating\" ] )\n",
    "        \n",
    "    return ratings.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the raw files\n",
    "generate_raw_files( \"./source\", \"./target\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cases tokens\n",
    "tokenized_cases = tokenize_files_in_path( \"./target/cases\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries tokens\n",
    "tokenized_summaries = tokenize_files_in_path( \"./target/summaries\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cases word2vec model\n",
    "cases_word2vec_model = create_word2vec_model( tokenized_cases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries word2vec model\n",
    "summaries_word2vec_model = create_word2vec_model( tokenized_summaries )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate general (cases + summaries ) word2vec model\n",
    "general_word2vec_model = create_word2vec_model( tokenized_cases + tokenized_summaries )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test word2vec models (check similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manslaughter', 'provocation', 'convicted', 'kidnapping', 'robbery']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the cases model\n",
    "print get_synonyms( cases_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kidnapping', 'violence', 'tried', 'robbery', 'guilty']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the summaries model\n",
    "print get_synonyms( summaries_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manslaughter', 'kidnapping', 'defilement', 'rape']\n"
     ]
    }
   ],
   "source": [
    "# Get synonyms based on the general model\n",
    "print get_synonyms( general_word2vec_model, \"murder\", 0.8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the cases\n",
    "scored_case_tokens = score_tokenized_sentences( tokenized_cases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the summaries\n",
    "scored_summary_tokens = score_tokenized_sentences( tokenized_summaries )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order the tokens by rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_summary_tokens = extract_ratings_from_scored_tokens( scored_summary_tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print scored_summary_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
